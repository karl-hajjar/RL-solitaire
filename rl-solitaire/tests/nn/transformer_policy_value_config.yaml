activation:
  approximate: tanh
  name: gelu
architecture:
  embeddings:
    bias: true
    hidden_dim: 128
    feedforward_hidden_dim: 256
    n_heads: 4
    input_dim: 3
    n_layers: 2
  policy_head:
    bias: true
    hidden_dim: 128
    feedforward_hidden_dim: 256
    n_heads: 4
    input_dim: 128
    n_layers: 1
    output_dim: 4
  value_head:
    bias: true
    hidden_dim: 128
    feedforward_hidden_dim: 256
    n_heads: 4
    input_dim: 128
    n_layers: 1
    output_dim: 1
initializer:
  name: he_normal
loss:
  actor_loss:
    coef: 5.0  # 1.8
    # name: ppo_clip
    label_smoothing: 0.000001
    name: cross_entropy
    reduction: none
  critic_loss:
    coef: 1.0
    name: mse
    reduction: mean
  regularization:
    coef: 1.0e-04
    name: entropy
name: TransformerPolicyValueNet
optimizer:
  lr: 3.0e-05
  name: adam
  weight_decay: 1.0e-05
